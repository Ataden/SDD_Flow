# Testing Plan Template

*Project:* [Project Name]
*Step:* [Implementation Step ID, e.g., 1.2]
*Date:* [Date of Document Creation]

---

## Overview

**Purpose:** Describe what this testing plan validates. Focus on verifying functionality, integration, and UI behavior for a specific implementation step.

**Scope:** Indicate what components are covered (e.g., service classes, API endpoints, frontend components, integrations).

**Exclusions:** Performance and security tests are excluded from this plan.

---

## 1. Test Pre-requisites

**Dependencies:**

* Backend server: [e.g., FastAPI at localhost:8000]
* Frontend client: [e.g., Next.js at localhost:3000]
* Additional services (if any): [e.g., Voice service at localhost:8001]

**Environment Variables:**

* `JWT_SECRET_KEY`
* `DATABASE_URL`
* Other relevant config values

**Test Accounts:**

* Username: \[test user]
* Password: \[test password]
* Token acquisition method: POST /api/auth/login

**Test Data:**

* Ensure consistent test DB state before test runs
* Populate required tables with valid test data

**Testing suite:**

* Unit tests [pytest, jest]
* Integration tests [pytest, jest]
* E2E tests [playwright]

**Test folders location:** 
All testing files are stored in the /test folder of a specific project (e.g., `frontend/test`, `backend/test`, etc). Then specific tests folders are:
* Unit tests - `test/unit`
* Integration tests - `test/integration`
* E2E tests - `test/e2e` for tests, `test/e2e/playwright-report` for reports

---

## 2. Unit Tests

### [Service/Class Name]

Each method/function:

#### UT-<service_id>.<service_name>-<testID> [Test Name]
Notes of naming: 
- <service_id>.<service_name> is a unique identifier from the TDDoc document
- <testID> must be unique for a given <service_id>.<service_name>. When adding a new test, always check for existing tests and increment <testID> to ensure it is unique.

example: `UT-B1.CallsService-01 should create call record with valid metadata` test must be place in `test/unit/UT_B1_CallsService_01.py`

File with tests: `tests/unit/UT-<service_id>.<service_name>.py`

* **Objective:** [What is being validated]
* **Setup/Input:** [Preconditions, data or state]
* **Expected Result:** [Outcome of the test]



(Repeat this structure for each Service/Class/Function/Method tested.)

---

## 3. Integration Tests

### API Endpoint Testing

For each relevant endpoint:

#### API-<service_id>.<service_name>-<testID> [Action/Route]
example: `API-B2.AuthService-01 Login with valid credentials`

File with tests: `tests/integration/API-<service_id>.<service_name>.py`

* **Method:** [GET/POST/etc.]
* **Endpoint:** [URL path]
* **Input:** [Request body or query params]
* **Expected Result:** [API response, DB change, etc.]


### Frontend-to-Backend Integration
For each integration point:

#### INT-<service>-<interactionID> [Name of the integration point/interaction point]
Note: InteractionID is a unique identifier from the TDDoc document.

File with tests: `tests/integration/INT-<service>-<interactionID>.py`

* **Objective:** [What is being validated]
* **Setup/Input:** [Preconditions, data or state]
* **Expected Result:** [Outcome of the test]

**Test Cases:**
* Data is loaded from the API and displayed correctly
* Errors (401, 500, etc.) are handled gracefully
* JWT token is included and validated properly
* Pagination and filtering parameters function as expected

### Other Integration Tests

---

## 4. End-to-End (E2E) Testing

#### Scenario [E2E-<scenarioID>]: [Scenario Name]

* **Steps:**
  1. [User/login/service action]
  2. [API or UI interaction]
  3. [Result validation]
* **Expected Result:** [What must happen at each step]

Repeat for each complete user-facing flow that crosses system boundaries (e.g., login → data fetch → UI update).

---

## 5. Regression Testing

Regression tests re-run a selected set of previously defined test cases to confirm that existing features still behave correctly after recent changes.

### Re-Tested Test Cases
Example:
```markdown
| Test Case ID     | Description                            | Original Location     | Status |
|------------------|----------------------------------------|------------------------|--------|
| UT-callservice-01| Should create call record              | Unit Test Section      | Passed |
| API-calls-03     | POST /api/calls/{id}/end               | API Endpoint Tests     | Passed |
| E2E-dashboard-01 | Dashboard displays recent calls        | End-to-End Scenarios   | Passed |
| INT-dashboard-02 | Frontend fetches paginated call data   | Frontend Integration   | Passed |
```

### Notes
- Focused on areas affected by the new [feature/refactor/bugfix]
- Run after completing new implementation and before deployment
- Failures here block release until resolved

---

## 6. Test Completion Criteria

**Functional:**

* All test cases in Unit, Integration, and E2E sections have passed
* Edge cases are handled correctly
* System behaves as expected with valid and invalid data

**Quality:**

* Clean and readable UI output (for frontend)
* Stable service behavior under typical user actions
* Proper error handling and messages

---

## 7. Test Summary Table

| Test Case ID | Description   | Expected Result     | Status    |
| ------------ | ------------- | ------------------- | --------- |
| TC-001       | \[Brief desc] | \[Expected outcome] | \[Passed] |
| TC-002       | ...           | ...                 | ...       |

---

## Notes

* All tests should be automated where possible
* Database state should be cleaned before and after tests



